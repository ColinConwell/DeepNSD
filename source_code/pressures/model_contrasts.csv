model,train_type,train_data,architecture,model_class,task_cluster,model_display_name,description,model_source,model_source_url,weights_url,model_string,compare_training,compare_architecture,compare_goal_slip,compare_goal_taskonomy_tasks,compare_goal_taskonomy_cluster,compare_goal_expertise,compare_diet_ipcl,compare_goal_selfsupervised,compare_goal_contrastive,compare_diet_imagenetsize
alexnet,classification,imagenet,alexnet,Convolutional,Semantic,AlexNet,AlexNet trained on image classification with the ImageNet dataset.,torchvision,pytorch.org/docs/stable/torchvision/models.html,,alexnet_classification,classification,Convolutional,,,,,,,,
vgg16,classification,imagenet,vgg16,Convolutional,Semantic,VGG16,VGG16 trained on image classification with the ImageNet dataset.,torchvision,pytorch.org/docs/stable/torchvision/models.html,,vgg16_classification,classification,Convolutional,,,,,,,,
resnet18,classification,imagenet,resnet18,Convolutional,Semantic,ResNet18,ResNet18 trained on image classification with the ImageNet dataset.,torchvision,pytorch.org/docs/stable/torchvision/models.html,,resnet18_classification,classification,Convolutional,,,,,,,,
resnet50,classification,imagenet,resnet50,Convolutional,Semantic,ResNet50,ResNet50 trained on image classification with the ImageNet dataset.,torchvision,pytorch.org/docs/stable/torchvision/models.html,,resnet50_classification,classification,Convolutional,,,,,,,,
resnet101,classification,imagenet,resnet101,Convolutional,Semantic,ResNet101,ResNet101 trained on image classification with the ImageNet dataset.,torchvision,pytorch.org/docs/stable/torchvision/models.html,,resnet101_classification,classification,Convolutional,,,,,,,,
resnet152,classification,imagenet,resnet152,Convolutional,Semantic,ResNet152,ResNet152 trained on image classification with the ImageNet dataset.,torchvision,pytorch.org/docs/stable/torchvision/models.html,,resnet152_classification,classification,Convolutional,,,,,,,,
squeezenet1_0,classification,imagenet,squeezenet1_0,Convolutional,Semantic,SqueezeNet1.0,SqueezeNet1.0 trained on image classification with the ImageNet dataset.,torchvision,pytorch.org/docs/stable/torchvision/models.html,,squeezenet1_0_classification,classification,Convolutional,,,,,,,,
densenet121,classification,imagenet,densenet121,Convolutional,Semantic,DenseNet121,DenseNet121 trained on image classification with the ImageNet dataset.,torchvision,pytorch.org/docs/stable/torchvision/models.html,,densenet121_classification,classification,Convolutional,,,,,,,,
googlenet,classification,imagenet,googlenet,Convolutional,Semantic,GoogleNet,GoogleNet trained on image classification with the ImageNet dataset.,torchvision,pytorch.org/docs/stable/torchvision/models.html,,googlenet_classification,classification,Convolutional,,,,,,,,
shufflenet_v2_x1_0,classification,imagenet,shufflenet_v2_x1_0,Convolutional,Semantic,ShuffleNet-V2-x1.0,ShuffleNet-V2-x1.0 trained on image classification with the ImageNet dataset.,torchvision,pytorch.org/docs/stable/torchvision/models.html,,shufflenet_v2_x1_0_classification,classification,Convolutional,,,,,,,,
mobilenet_v2,classification,imagenet,mobilenet_v2,Convolutional,Semantic,MobileNet-V2,MobileNet-V2 trained on image classification with the ImageNet dataset.,torchvision,pytorch.org/docs/stable/torchvision/models.html,,mobilenet_v2_classification,classification,Convolutional,,,,,,,,
mnasnet1_0,classification,imagenet,mnasnet1_0,Convolutional,Semantic,MNASNet1.0,MNASNet1.0 trained on image classification with the ImageNet dataset.,torchvision,pytorch.org/docs/stable/torchvision/models.html,,mnasnet1_0_classification,classification,Convolutional,,,,,,,,
alexnet,random,,alexnet,Convolutional,Semantic,AlexNet,"AlexNet randomly initialized, with no training.",torchvision,pytorch.org/docs/stable/torchvision/models.html,,alexnet_random,random,,,,,,,,,
vgg16,random,,vgg16,Convolutional,Semantic,VGG16,"VGG16 randomly initialized, with no training.",torchvision,pytorch.org/docs/stable/torchvision/models.html,,vgg16_random,random,,,,,,,,,
resnet18,random,,resnet18,Convolutional,Semantic,ResNet18,"ResNet18 randomly initialized, with no training.",torchvision,pytorch.org/docs/stable/torchvision/models.html,,resnet18_random,random,,,,,,,,,
resnet50,random,,resnet50,Convolutional,Semantic,ResNet50,"ResNet50 randomly initialized, with no training.",torchvision,pytorch.org/docs/stable/torchvision/models.html,,resnet50_random,random,,,,,,,,,
resnet101,random,,resnet101,Convolutional,Semantic,ResNet101,"ResNet101 randomly initialized, with no training.",torchvision,pytorch.org/docs/stable/torchvision/models.html,,resnet101_random,random,,,,,,,,,
resnet152,random,,resnet152,Convolutional,Semantic,ResNet152,"ResNet152 randomly initialized, with no training.",torchvision,pytorch.org/docs/stable/torchvision/models.html,,resnet152_random,random,,,,,,,,,
squeezenet1_0,random,,squeezenet1_0,Convolutional,Semantic,SqueezeNet1.0,"SqueezeNet1.0 randomly initialized, with no training.",torchvision,pytorch.org/docs/stable/torchvision/models.html,,squeezenet1_0_random,random,,,,,,,,,
densenet121,random,,densenet121,Convolutional,Semantic,DenseNet121,"DenseNet121 randomly initialized, with no training.",torchvision,pytorch.org/docs/stable/torchvision/models.html,,densenet121_random,random,,,,,,,,,
googlenet,random,,googlenet,Convolutional,Semantic,GoogleNet,"GoogleNet randomly initialized, with no training.",torchvision,pytorch.org/docs/stable/torchvision/models.html,,googlenet_random,random,,,,,,,,,
shufflenet_v2_x1_0,random,,shufflenet_v2_x1_0,Convolutional,Semantic,ShuffleNet-V2-x1.0,"ShuffleNet-V2-x1.0 randomly initialized, with no training.",torchvision,pytorch.org/docs/stable/torchvision/models.html,,shufflenet_v2_x1_0_random,random,,,,,,,,,
mobilenet_v2,random,,mobilenet_v2,Convolutional,Semantic,MobileNet-V2,"MobileNet-V2 randomly initialized, with no training.",torchvision,pytorch.org/docs/stable/torchvision/models.html,,mobilenet_v2_random,random,,,,,,,,,
mnasnet1_0,random,,mnasnet1_0,Convolutional,Semantic,MNASNet1.0,"MNASNet1.0 randomly initialized, with no training.",torchvision,pytorch.org/docs/stable/torchvision/models.html,,mnasnet1_0_random,random,,,,,,,,,
autoencoding,taskonomy,taskonomy,resnet50,Convolutional,2D,Autoencoder,Image compression and decompression,taskonomy,github.com/alexsax/midlevel-reps/tree/visualpriors,,autoencoding_taskonomy,,,,autoencoding,2D,,,,,
class_object,taskonomy,taskonomy,resnet50,Convolutional,Semantic,Object Classification,1000-way object classification (via knowledge distillation from ImageNet).,taskonomy,github.com/alexsax/midlevel-reps/tree/visualpriors,,class_object_taskonomy,,,,class_object,Semantic,,,,,
class_scene,taskonomy,taskonomy,resnet50,Convolutional,Semantic,Scene Classification,Scene Classification (via knowledge distillation from MIT Places).,taskonomy,github.com/alexsax/midlevel-reps/tree/visualpriors,,class_scene_taskonomy,,,,class_scene,Semantic,,,,,
curvature,taskonomy,taskonomy,resnet50,Convolutional,3D,Curvatures,Magnitude of 3D principal curvatures,taskonomy,github.com/alexsax/midlevel-reps/tree/visualpriors,,curvature_taskonomy,,,,curvature,3D,,,,,
denoising,taskonomy,taskonomy,resnet50,Convolutional,Other,Denoising,Uncorrupted version of corrupted image.,taskonomy,github.com/alexsax/midlevel-reps/tree/visualpriors,,denoising_taskonomy,,,,denoising,Other,,,,,
depth_euclidean,taskonomy,taskonomy,resnet50,Convolutional,3D,Euclidean Depth,Depth estimation,taskonomy,github.com/alexsax/midlevel-reps/tree/visualpriors,,depth_euclidean_taskonomy,,,,depth_euclidean,3D,,,,,
depth_zbuffer,taskonomy,taskonomy,resnet50,Convolutional,3D,Z-Buffer Depth,Depth estimation.,taskonomy,github.com/alexsax/midlevel-reps/tree/visualpriors,,depth_zbuffer_taskonomy,,,,depth_zbuffer,3D,,,,,
edge_occlusion,taskonomy,taskonomy,resnet50,Convolutional,3D,Occlusion Edges,Edges which include parts of the scene.,taskonomy,github.com/alexsax/midlevel-reps/tree/visualpriors,,edge_occlusion_taskonomy,,,,edge_occlusion,3D,,,,,
edge_texture,taskonomy,taskonomy,resnet50,Convolutional,2D,Texture Edges,Edges computed from RGB only (texture edges).,taskonomy,github.com/alexsax/midlevel-reps/tree/visualpriors,,edge_texture_taskonomy,,,,edge_texture,2D,,,,,
egomotion,taskonomy,taskonomy,resnet50,Convolutional,Geometric,Egomotion,Odometry (camera poses) given three input images.,taskonomy,github.com/alexsax/midlevel-reps/tree/visualpriors,,egomotion_taskonomy,,,,egomotion,Geometric,,,,,
fixated_pose,taskonomy,taskonomy,resnet50,Convolutional,Geometric,Camera Pose (Fixated),Relative camera pose with matching optical centers.,taskonomy,github.com/alexsax/midlevel-reps/tree/visualpriors,,fixated_pose_taskonomy,,,,fixated_pose,Geometric,,,,,
inpainting,taskonomy,taskonomy,resnet50,Convolutional,2D,Inpainting,Filling in masked center of image.,taskonomy,github.com/alexsax/midlevel-reps/tree/visualpriors,,inpainting_taskonomy,,,,inpainting,2D,,,,,
jigsaw,taskonomy,taskonomy,resnet50,Convolutional,Geometric,Jigsaw,Putting scrambled image pieces back together.,taskonomy,github.com/alexsax/midlevel-reps/tree/visualpriors,,jigsaw_taskonomy,,,,jigsaw,Geometric,,,,,
keypoints2d,taskonomy,taskonomy,resnet50,Convolutional,2D,2D Keypoints,Keypoint estimation from RGB-only (texture features).,taskonomy,github.com/alexsax/midlevel-reps/tree/visualpriors,,keypoints2d_taskonomy,,,,keypoints2d,2D,,,,,
keypoints3d,taskonomy,taskonomy,resnet50,Convolutional,3D,3D Keypoints,3D Keypoint estimation from underlying scene 3D.,taskonomy,github.com/alexsax/midlevel-reps/tree/visualpriors,,keypoints3d_taskonomy,,,,keypoints3d,3D,,,,,
nonfixated_pose,taskonomy,taskonomy,resnet50,Convolutional,Geometric,Camera Pose (Nonfixated),Relative camera pose with distinct optical centers.,taskonomy,github.com/alexsax/midlevel-reps/tree/visualpriors,,nonfixated_pose_taskonomy,,,,nonfixated_pose,Geometric,,,,,
normal,taskonomy,taskonomy,resnet50,Convolutional,3D,Surface Normals,Pixel-wise surface normals.,taskonomy,github.com/alexsax/midlevel-reps/tree/visualpriors,,normal_taskonomy,,,,normal,3D,,,,,
point_matching,taskonomy,taskonomy,resnet50,Convolutional,Geometric,Point Matching,Classifying if centers of two images match or not.,taskonomy,github.com/alexsax/midlevel-reps/tree/visualpriors,,point_matching_taskonomy,,,,point_matching,Geometric,,,,,
reshading,taskonomy,taskonomy,resnet50,Convolutional,3D,Reshading,Reshading with new lighting placed at camera location.,taskonomy,github.com/alexsax/midlevel-reps/tree/visualpriors,,reshading_taskonomy,,,,reshading,3D,,,,,
room_layout,taskonomy,taskonomy,resnet50,Convolutional,Geometric,Room Layout,Orientation and aspect ratio of cubic room layout.,taskonomy,github.com/alexsax/midlevel-reps/tree/visualpriors,,room_layout_taskonomy,,,,room_layout,Geometric,,,,,
segment_semantic,taskonomy,taskonomy,resnet50,Convolutional,Semantic,Semantic Segmentation,Pixel-wise semantic labeling (via knowledge distillation from MS COCO).,taskonomy,github.com/alexsax/midlevel-reps/tree/visualpriors,,segment_semantic_taskonomy,,,,segment_semantic,Semantic,,,,,
segment_unsup25d,taskonomy,taskonomy,resnet50,Convolutional,3D,Unsupervised 2.5D Segmentation,Segmentation (graph cut approximation) on RGB-D-Normals-Curvature image.,taskonomy,github.com/alexsax/midlevel-reps/tree/visualpriors,,segment_unsup25d_taskonomy,,,,segment_unsup25d,3D,,,,,
segment_unsup2d,taskonomy,taskonomy,resnet50,Convolutional,2D,Unsupervised 2D Segmentation,Segmentation (graph cut approximation) on RGB.,taskonomy,github.com/alexsax/midlevel-reps/tree/visualpriors,,segment_unsup2d_taskonomy,,,,segment_unsup2d,2D,,,,,
vanishing_point,taskonomy,taskonomy,resnet50,Convolutional,Geometric,Vanishing Point,Three Manhattan-world vanishing points.,taskonomy,github.com/alexsax/midlevel-reps/tree/visualpriors,,vanishing_point_taskonomy,,,,vanishing_point,Geometric,,,,,
random_weights,taskonomy,,resnet50,Convolutional,Random,Random Weights,Taskonomy architecture randomly initialized.,taskonomy,github.com/alexsax/midlevel-reps/tree/visualpriors,,random_weights_taskonomy,,,,random_weights,Random,,,,,
coat_lite_tiny,classification,imagenet,coat_lite_tiny,Transformer,Semantic,CoaT-Lite-Tiny,CoaT-Lite-Tiny trained on image classification with the ImageNet dataset.,timm,https://github.com/rwightman/pytorch-image-models/,,coat_lite_tiny_classification,classification,Transformer,,,,,,,,
convit_base,classification,imagenet,convit_base,Hybrid,Semantic,ConViT-B,ConViT-B trained on image classification with the ImageNet dataset.,timm,https://github.com/rwightman/pytorch-image-models/,,convit_base_classification,classification,Hybrid,,,,,,,,
convit_tiny,classification,imagenet,convit_tiny,Hybrid,Semantic,ConViT-T,ConViT-T trained on image classification with the ImageNet dataset.,timm,https://github.com/rwightman/pytorch-image-models/,,convit_tiny_classification,classification,Hybrid,,,,,,,,
convmixer_768_32,classification,imagenet,convmixer_768_32,Hybrid,Semantic,ConvMixer-768-32,ConvMixer-768-32 trained on image classification with the ImageNet dataset.,timm,https://github.com/rwightman/pytorch-image-models/,,convmixer_768_32_classification,classification,Hybrid,,,,,,,,
convnext_base,classification,imagenet,convnext_base,Convolutional,Semantic,ConvNext-B,ConvNext-B trained on image classification with the ImageNet dataset.,timm,https://github.com/rwightman/pytorch-image-models/,,convnext_base_classification,classification,Convolutional,,,,,,,,imagenet
convnext_large,classification,imagenet,convnext_large,Convolutional,Semantic,ConvNext-L,ConvNext-L trained on image classification with the ImageNet dataset.,timm,https://github.com/rwightman/pytorch-image-models/,,convnext_large_classification,classification,Convolutional,,,,,,,,imagenet
crossvit_base_240,classification,imagenet,crossvit_base_240,Transformer,Semantic,CrossViT-B,CrossViT-B trained on image classification with the ImageNet dataset.,timm,https://github.com/rwightman/pytorch-image-models/,,crossvit_base_240_classification,classification,Transformer,,,,,,,,
deit_base_patch16_224,classification,imagenet,deit_base_patch16_224,Transformer,Semantic,DeiT-B-P16-224,DeiT-B-P16-224 trained on image classification with the ImageNet dataset.,timm,https://github.com/rwightman/pytorch-image-models/,,deit_base_patch16_224_classification,classification,Transformer,,,,,,,,
cspresnet50,classification,imagenet,cspresnet50,Convolutional,Semantic,CSP-ResNet50,CSP-ResNet50 trained on image classification with the ImageNet dataset.,timm,https://github.com/rwightman/pytorch-image-models/,,cspresnet50_classification,classification,Convolutional,,,,,,,,
dla34,classification,imagenet,dla34,Convolutional,Semantic,DLA34,DLA34 trained on image classification with the ImageNet dataset.,timm,https://github.com/rwightman/pytorch-image-models/,,dla34_classification,classification,Convolutional,,,,,,,,
eca_nfnet_l0,classification,imagenet,eca_nfnet_l0,Convolutional,Semantic,ECA-NFNeT-L0,ECA-NFNeT-L0 trained on image classification with the ImageNet dataset.,timm,https://github.com/rwightman/pytorch-image-models/,,eca_nfnet_l0_classification,classification,Convolutional,,,,,,,,
efficientnet_b1,classification,imagenet,efficientnet_b1,Convolutional,Semantic,EfficientNet-B1,EfficientNet-B1 trained on image classification with the ImageNet dataset.,timm,https://github.com/rwightman/pytorch-image-models/,,efficientnet_b1_classification,classification,Convolutional,,,,,,,,
efficientnet_b3,classification,imagenet,efficientnet_b3,Convolutional,Semantic,EfficientNet-B3,EfficientNet-B3 trained on image classification with the ImageNet dataset.,timm,https://github.com/rwightman/pytorch-image-models/,,efficientnet_b3_classification,classification,Convolutional,,,,,,,,
ghostnet_100,classification,imagenet,ghostnet_100,Convolutional,Semantic,GhostNet100,GhostNet100 trained on image classification with the ImageNet dataset.,timm,https://github.com/rwightman/pytorch-image-models/,,ghostnet_100_classification,classification,Convolutional,,,,,,,,
gmixer_24_224,classification,imagenet,gmixer_24_224,Convolutional,Semantic,GMixer-24,GMixer-24 trained on image classification with the ImageNet dataset.,timm,https://github.com/rwightman/pytorch-image-models/,,gmixer_24_224_classification,classification,Convolutional,,,,,,,,
gmlp_s16_224,classification,imagenet,gmlp_s16_224,Convolutional,Semantic,GMLP-S16,GMLP-S16 trained on image classification with the ImageNet dataset.,timm,https://github.com/rwightman/pytorch-image-models/,,gmlp_s16_224_classification,classification,Convolutional,,,,,,,,
hardcorenas_a,classification,imagenet,hardcorenas_a,Convolutional,Semantic,HardCoreNAS-A,HardCoreNAS-A trained on image classification with the ImageNet dataset.,timm,https://github.com/rwightman/pytorch-image-models/,,hardcorenas_a_classification,classification,Convolutional,,,,,,,,
hardcorenas_f,classification,imagenet,hardcorenas_f,Convolutional,Semantic,HardCoreNAS-F,HardCoreNAS-F trained on image classification with the ImageNet dataset.,timm,https://github.com/rwightman/pytorch-image-models/,,hardcorenas_f_classification,classification,Convolutional,,,,,,,,
jx_nest_tiny,classification,imagenet,jx_nest_tiny,Transformer,Semantic,JX-NesT-Tiny,JX-NesT-Tiny trained on image classification with the ImageNet dataset.,timm,https://github.com/rwightman/pytorch-image-models/,,jx_nest_tiny_classification,classification,Transformer,,,,,,,,
levit_128,classification,imagenet,levit_128,Transformer,Semantic,LeViT128,LeViT128 trained on image classification with the ImageNet dataset.,timm,https://github.com/rwightman/pytorch-image-models/,,levit_128_classification,classification,Transformer,,,,,,,,
inception_v3,classification,imagenet,inception_v3,Convolutional,Semantic,Inception-V3,Inception-V3 trained on image classification with the ImageNet dataset.,timm,https://github.com/rwightman/pytorch-image-models/,,inception_v3_classification,classification,Convolutional,,,,,,,,
mixer_b16_224,classification,imagenet,mixer_b16_224,MLP-Mixer,Semantic,MLP-Mixer-B16,MLP-Mixer-B16 trained on image classification with the ImageNet dataset.,timm,https://github.com/rwightman/pytorch-image-models/,,mixer_b16_224_classification,classification,MLP-Mixer,,,,,,,,imagenet
mixer_l16_224,classification,imagenet,mixer_l16_224,MLP-Mixer,Semantic,MLP-Mixer-L16,MLP-Mixer-L16 trained on image classification with the ImageNet dataset.,timm,https://github.com/rwightman/pytorch-image-models/,,mixer_l16_224_classification,classification,MLP-Mixer,,,,,,,,imagenet
mobilenetv3_large_100,classification,imagenet,mobilenetv3_large_100,Convolutional,Semantic,MobileNet-V3-Large,MobileNet-V3-Large trained on image classification with the ImageNet dataset.,timm,https://github.com/rwightman/pytorch-image-models/,,mobilenetv3_large_100_classification,classification,Convolutional,,,,,,,,
nf_resnet50,classification,imagenet,nf_resnet50,Convolutional,Semantic,NF-ResNet50,NF-ResNet50 trained on image classification with the ImageNet dataset.,timm,https://github.com/rwightman/pytorch-image-models/,,nf_resnet50_classification,classification,Convolutional,,,,,,,,
nfnet_l0,classification,imagenet,nfnet_l0,Convolutional,Semantic,NF-Net-L0,NF-Net-L0 trained on image classification with the ImageNet dataset.,timm,https://github.com/rwightman/pytorch-image-models/,,nfnet_l0_classification,classification,Convolutional,,,,,,,,
pit_b_224,classification,imagenet,pit_b_224,Transformer,Semantic,PiT-B-224,PiT-B-224 trained on image classification with the ImageNet dataset.,timm,https://github.com/rwightman/pytorch-image-models/,,pit_b_224_classification,classification,Transformer,,,,,,,,
pit_ti_224,classification,imagenet,pit_ti_224,Transformer,Semantic,PiT-T-224,PiT-T-224 trained on image classification with the ImageNet dataset.,timm,https://github.com/rwightman/pytorch-image-models/,,pit_ti_224_classification,classification,Transformer,,,,,,,,
poolformer_s36,classification,imagenet,poolformer_s36,Transformer,Semantic,PoolFormer-S36,PoolFormer-S36 trained on image classification with the ImageNet dataset.,timm,https://github.com/rwightman/pytorch-image-models/,,poolformer_s36_classification,classification,Transformer,,,,,,,,
pit_ti_224,classification,imagenet,pit_ti_224,Transformer,Semantic,PiT-T-224,PiT-T-224 trained on image classification with the ImageNet dataset.,timm,https://github.com/rwightman/pytorch-image-models/,,pit_ti_224_classification,classification,Transformer,,,,,,,,
regnetx_064,classification,imagenet,regnetx_064,Convolutional,Semantic,RegNetX-64,RegNetX-64 trained on image classification with the ImageNet dataset.,timm,https://github.com/rwightman/pytorch-image-models/,,regnetx_064_classification,classification,Convolutional,,,,,,,,
regnety_064,classification,imagenet,regnety_064,Convolutional,Semantic,RegNetY-64,RegNetY-64 trained on image classification with the ImageNet dataset.,timm,https://github.com/rwightman/pytorch-image-models/,,regnety_064_classification,classification,Convolutional,,,,,,,,
resmlp_12_224,classification,imagenet,resmlp_12_224,MLP-Mixer,Semantic,ResMLP-12,ResMLP-12 trained on image classification with the ImageNet dataset.,timm,https://github.com/rwightman/pytorch-image-models/,,resmlp_12_224_classification,classification,MLP-Mixer,,,,,,,,
resmlp_24_224,classification,imagenet,resmlp_24_224,MLP-Mixer,Semantic,ResMLP-24,ResMLP-24 trained on image classification with the ImageNet dataset.,timm,https://github.com/rwightman/pytorch-image-models/,,resmlp_24_224_classification,classification,MLP-Mixer,,,,,,,,
resmlp_36_224,classification,imagenet,resmlp_36_224,MLP-Mixer,Semantic,ResMLP-36,ResMLP-36 trained on image classification with the ImageNet dataset.,timm,https://github.com/rwightman/pytorch-image-models/,,resmlp_36_224_classification,classification,MLP-Mixer,,,,,,,,
resmlp_big_24_224,classification,imagenet,resmlp_big_24_224,MLP-Mixer,Semantic,ResMLP-Big-24,ResMLP-Big-24 trained on image classification with the ImageNet dataset.,timm,https://github.com/rwightman/pytorch-image-models/,,resmlp_big_24_224_classification,classification,MLP-Mixer,,,,,,,,imagenet
semnasnet_100,classification,imagenet,semnasnet_100,Convolutional,Semantic,SemNASNet100,SemNASNet100 trained on image classification with the ImageNet dataset.,timm,https://github.com/rwightman/pytorch-image-models/,,semnasnet_100_classification,classification,Convolutional,,,,,,,,
seresnext50_32x4d,classification,imagenet,seresnext50_32x4d,Convolutional,Semantic,SEResNext50-32x4D,SEResNext50-32x4D trained on image classification with the ImageNet dataset.,timm,https://github.com/rwightman/pytorch-image-models/,,seresnext50_32x4d_classification,classification,Convolutional,,,,,,,,
skresnext50_32x4d,classification,imagenet,skresnext50_32x4d,Convolutional,Semantic,SKResNext50-32x4D,SKResNext50-32x4D trained on image classification with the ImageNet dataset.,timm,https://github.com/rwightman/pytorch-image-models/,,skresnext50_32x4d_classification,classification,Convolutional,,,,,,,,
swin_base_patch4_window7_224,classification,imagenet,swin_base_patch4_window7_224,Transformer,Semantic,Swin-B-P4-W7,Swin-B-P4-W7 trained on image classification with the ImageNet dataset.,timm,https://github.com/rwightman/pytorch-image-models/,,swin_base_patch4_window7_224_classification,classification,Transformer,,,,,,,,imagenet
swin_large_patch4_window7_224,classification,imagenet,swin_large_patch4_window7_224,Transformer,Semantic,Swin-L-P4-W7,Swin-L-P4-W7 trained on image classification with the ImageNet dataset.,timm,https://github.com/rwightman/pytorch-image-models/,,swin_large_patch4_window7_224_classification,classification,Transformer,,,,,,,,imagenet
swin_tiny_patch4_window7_224,classification,imagenet,swin_tiny_patch4_window7_224,Transformer,Semantic,Swin-T-P4-W7,Swin-T-P4-W7 trained on image classification with the ImageNet dataset.,timm,https://github.com/rwightman/pytorch-image-models/,,swin_tiny_patch4_window7_224_classification,classification,Transformer,,,,,,,,
tnt_s_patch16_224,classification,imagenet,tnt_s_patch16_224,Transformer,Semantic,TnT-P16-224,TnT-P16-224 trained on image classification with the ImageNet dataset.,timm,https://github.com/rwightman/pytorch-image-models/,,tnt_s_patch16_224_classification,classification,Transformer,,,,,,,,
visformer_small,classification,imagenet,visformer_small,Transformer,Semantic,Visformer,Visformer trained on image classification with the ImageNet dataset.,timm,https://github.com/rwightman/pytorch-image-models/,,visformer_small_classification,classification,Transformer,,,,,,,,
vit_large_patch16_224,classification,imagenet,vit_large_patch16_224,Transformer,Semantic,ViT-L-P16,ViT-L-P16 trained on image classification with the ImageNet dataset.,timm,https://github.com/rwightman/pytorch-image-models/,,vit_large_patch16_224_classification,classification,Transformer,,,,,,,,imagenet
vit_small_patch16_224,classification,imagenet,vit_small_patch16_224,Transformer,Semantic,ViT-S-P16,ViT-S-P16 trained on image classification with the ImageNet dataset.,timm,https://github.com/rwightman/pytorch-image-models/,,vit_small_patch16_224_classification,classification,Transformer,,,,,,,,imagenet
vit_tiny_patch16_224,classification,imagenet,vit_tiny_patch16_224,Transformer,Semantic,ViT-T-P16,ViT-T-P16 trained on image classification with the ImageNet dataset.,timm,https://github.com/rwightman/pytorch-image-models/,,vit_tiny_patch16_224_classification,classification,Transformer,,,,,,,,
vit_base_patch16_224,classification,imagenet,vit_base_patch16_224,Transformer,Semantic,ViT-B-P16,ViT-B-P16 trained on image classification with the ImageNet dataset.,timm,https://github.com/rwightman/pytorch-image-models/,,vit_base_patch16_224_classification,classification,Transformer,,,,,,,,imagenet
vit_base_patch32_224,classification,imagenet,vit_base_patch32_224,Transformer,Semantic,ViT-B-P32,ViT-B-P32 trained on image classification with the ImageNet dataset.,timm,https://github.com/rwightman/pytorch-image-models/,,vit_base_patch32_224_classification,classification,Transformer,,,,,,,,imagenet
vit_small_patch32_224,classification,imagenet,vit_small_patch32_224,Transformer,Semantic,ViT-S-P32,ViT-S-P32 trained on image classification with the ImageNet dataset.,timm,https://github.com/rwightman/pytorch-image-models/,,vit_small_patch32_224_classification,classification,Transformer,,,,,,,,imagenet
xception,classification,imagenet,xception,Convolutional,Semantic,XCeption,XCeption trained on image classification with the ImageNet dataset.,timm,https://github.com/rwightman/pytorch-image-models/,,xception_classification,classification,Convolutional,,,,,,,,
xcit_nano_12_p8_224,classification,imagenet,xcit_nano_12_p8_224,Transformer,Semantic,XCIT-N-12-P8,XCIT-N-12-P8 trained on image classification with the ImageNet dataset.,timm,https://github.com/rwightman/pytorch-image-models/,,xcit_nano_12_p8_224_classification,classification,Transformer,,,,,,,,
xcit_nano_12_p16_224,classification,imagenet,xcit_nano_12_p16_224,Transformer,Semantic,XCIT-N-12-P16,XCIT-N-12-P16 trained on image classification with the ImageNet dataset.,timm,https://github.com/rwightman/pytorch-image-models/,,xcit_nano_12_p16_224_classification,classification,Transformer,,,,,,,,
coat_lite_tiny,random,,coat_lite_tiny,Transformer,Semantic,CoaT-Lite-Tiny,"CoaT-Lite-Tiny randomly initialized, with no training.",timm,https://github.com/rwightman/pytorch-image-models/,,coat_lite_tiny_random,random,,,,,,,,,
convit_base,random,,convit_base,Hybrid,Semantic,ConViT-B,"ConViT-B randomly initialized, with no training.",timm,https://github.com/rwightman/pytorch-image-models/,,convit_base_random,random,,,,,,,,,
convit_tiny,random,,convit_tiny,Hybrid,Semantic,ConViT-T,"ConViT-T randomly initialized, with no training.",timm,https://github.com/rwightman/pytorch-image-models/,,convit_tiny_random,random,,,,,,,,,
convmixer_768_32,random,,convmixer_768_32,Hybrid,Semantic,ConvMixer-768-32,"ConvMixer-768-32 randomly initialized, with no training.",timm,https://github.com/rwightman/pytorch-image-models/,,convmixer_768_32_random,random,,,,,,,,,
convnext_base,random,,convnext_base,Convolutional,Semantic,ConvNext-B,"ConvNext-B randomly initialized, with no training.",timm,https://github.com/rwightman/pytorch-image-models/,,convnext_base_random,random,,,,,,,,,
convnext_large,random,,convnext_large,Convolutional,Semantic,ConvNext-L,"ConvNext-L randomly initialized, with no training.",timm,https://github.com/rwightman/pytorch-image-models/,,convnext_large_random,random,,,,,,,,,
crossvit_base_240,random,,crossvit_base_240,Transformer,Semantic,CrossViT-B,"CrossViT-B randomly initialized, with no training.",timm,https://github.com/rwightman/pytorch-image-models/,,crossvit_base_240_random,random,,,,,,,,,
deit_base_patch16_224,random,,deit_base_patch16_224,Transformer,Semantic,DeiT-B-P16-224,"DeiT-B-P16-224 randomly initialized, with no training.",timm,https://github.com/rwightman/pytorch-image-models/,,deit_base_patch16_224_random,random,,,,,,,,,
cspresnet50,random,,cspresnet50,Convolutional,Semantic,CSP-ResNet50,"CSP-ResNet50 randomly initialized, with no training.",timm,https://github.com/rwightman/pytorch-image-models/,,cspresnet50_random,random,,,,,,,,,
dla34,random,,dla34,Convolutional,Semantic,DLA34,"DLA34 randomly initialized, with no training.",timm,https://github.com/rwightman/pytorch-image-models/,,dla34_random,random,,,,,,,,,
eca_nfnet_l0,random,,eca_nfnet_l0,Convolutional,Semantic,ECA-NFNeT-L0,"ECA-NFNeT-L0 randomly initialized, with no training.",timm,https://github.com/rwightman/pytorch-image-models/,,eca_nfnet_l0_random,random,,,,,,,,,
efficientnet_b1,random,,efficientnet_b1,Convolutional,Semantic,EfficientNet-B1,"EfficientNet-B1 randomly initialized, with no training.",timm,https://github.com/rwightman/pytorch-image-models/,,efficientnet_b1_random,random,,,,,,,,,
efficientnet_b3,random,,efficientnet_b3,Convolutional,Semantic,EfficientNet-B3,"EfficientNet-B3 randomly initialized, with no training.",timm,https://github.com/rwightman/pytorch-image-models/,,efficientnet_b3_random,random,,,,,,,,,
ghostnet_100,random,,ghostnet_100,Convolutional,Semantic,GhostNet100,"GhostNet100 randomly initialized, with no training.",timm,https://github.com/rwightman/pytorch-image-models/,,ghostnet_100_random,random,,,,,,,,,
gmixer_24_224,random,,gmixer_24_224,Convolutional,Semantic,GMixer-24,"GMixer-24 randomly initialized, with no training.",timm,https://github.com/rwightman/pytorch-image-models/,,gmixer_24_224_random,random,,,,,,,,,
gmlp_s16_224,random,,gmlp_s16_224,Convolutional,Semantic,GMLP-S16,"GMLP-S16 randomly initialized, with no training.",timm,https://github.com/rwightman/pytorch-image-models/,,gmlp_s16_224_random,random,,,,,,,,,
hardcorenas_a,random,,hardcorenas_a,Convolutional,Semantic,HardCoreNAS-A,"HardCoreNAS-A randomly initialized, with no training.",timm,https://github.com/rwightman/pytorch-image-models/,,hardcorenas_a_random,random,,,,,,,,,
hardcorenas_f,random,,hardcorenas_f,Convolutional,Semantic,HardCoreNAS-F,"HardCoreNAS-F randomly initialized, with no training.",timm,https://github.com/rwightman/pytorch-image-models/,,hardcorenas_f_random,random,,,,,,,,,
jx_nest_tiny,random,,jx_nest_tiny,Transformer,Semantic,JX-NesT-Tiny,"JX-NesT-Tiny randomly initialized, with no training.",timm,https://github.com/rwightman/pytorch-image-models/,,jx_nest_tiny_random,random,,,,,,,,,
levit_128,random,,levit_128,Transformer,Semantic,LeViT128,"LeViT128 randomly initialized, with no training.",timm,https://github.com/rwightman/pytorch-image-models/,,levit_128_random,random,,,,,,,,,
inception_v3,random,,inception_v3,Convolutional,Semantic,Inception-V3,"Inception-V3 randomly initialized, with no training.",timm,https://github.com/rwightman/pytorch-image-models/,,inception_v3_random,random,,,,,,,,,
mixer_b16_224,random,,mixer_b16_224,MLP-Mixer,Semantic,MLP-Mixer-B16,"MLP-Mixer-B16 randomly initialized, with no training.",timm,https://github.com/rwightman/pytorch-image-models/,,mixer_b16_224_random,random,,,,,,,,,
mixer_l16_224,random,,mixer_l16_224,MLP-Mixer,Semantic,MLP-Mixer-L16,"MLP-Mixer-L16 randomly initialized, with no training.",timm,https://github.com/rwightman/pytorch-image-models/,,mixer_l16_224_random,random,,,,,,,,,
mobilenetv3_large_100,random,,mobilenetv3_large_100,Convolutional,Semantic,MobileNet-V3-Large,"MobileNet-V3-Large randomly initialized, with no training.",timm,https://github.com/rwightman/pytorch-image-models/,,mobilenetv3_large_100_random,random,,,,,,,,,
nf_resnet50,random,,nf_resnet50,Convolutional,Semantic,NF-ResNet50,"NF-ResNet50 randomly initialized, with no training.",timm,https://github.com/rwightman/pytorch-image-models/,,nf_resnet50_random,random,,,,,,,,,
nfnet_l0,random,,nfnet_l0,Convolutional,Semantic,NF-Net-L0,"NF-Net-L0 randomly initialized, with no training.",timm,https://github.com/rwightman/pytorch-image-models/,,nfnet_l0_random,random,,,,,,,,,
pit_b_224,random,,pit_b_224,Transformer,Semantic,PiT-B-224,"PiT-B-224 randomly initialized, with no training.",timm,https://github.com/rwightman/pytorch-image-models/,,pit_b_224_random,random,,,,,,,,,
pit_ti_224,random,,pit_ti_224,Transformer,Semantic,PiT-T-224,"PiT-T-224 randomly initialized, with no training.",timm,https://github.com/rwightman/pytorch-image-models/,,pit_ti_224_random,random,,,,,,,,,
poolformer_s36,random,,poolformer_s36,Transformer,Semantic,PoolFormer-S36,"PoolFormer-S36 randomly initialized, with no training.",timm,https://github.com/rwightman/pytorch-image-models/,,poolformer_s36_random,random,,,,,,,,,
pit_ti_224,random,,pit_ti_224,Transformer,Semantic,PiT-T-224,"PiT-T-224 randomly initialized, with no training.",timm,https://github.com/rwightman/pytorch-image-models/,,pit_ti_224_random,random,,,,,,,,,
regnetx_064,random,,regnetx_064,Convolutional,Semantic,RegNetX-64,"RegNetX-64 randomly initialized, with no training.",timm,https://github.com/rwightman/pytorch-image-models/,,regnetx_064_random,random,,,,,,,,,
regnety_064,random,,regnety_064,Convolutional,Semantic,RegNetY-64,"RegNetY-64 randomly initialized, with no training.",timm,https://github.com/rwightman/pytorch-image-models/,,regnety_064_random,random,,,,,,,,,
resmlp_12_224,random,,resmlp_12_224,MLP-Mixer,Semantic,ResMLP-12,"ResMLP-12 randomly initialized, with no training.",timm,https://github.com/rwightman/pytorch-image-models/,,resmlp_12_224_random,random,,,,,,,,,
resmlp_24_224,random,,resmlp_24_224,MLP-Mixer,Semantic,ResMLP-24,"ResMLP-24 randomly initialized, with no training.",timm,https://github.com/rwightman/pytorch-image-models/,,resmlp_24_224_random,random,,,,,,,,,
resmlp_36_224,random,,resmlp_36_224,MLP-Mixer,Semantic,ResMLP-36,"ResMLP-36 randomly initialized, with no training.",timm,https://github.com/rwightman/pytorch-image-models/,,resmlp_36_224_random,random,,,,,,,,,
resmlp_big_24_224,random,,resmlp_big_24_224,MLP-Mixer,Semantic,ResMLP-Big-24,"ResMLP-Big-24 randomly initialized, with no training.",timm,https://github.com/rwightman/pytorch-image-models/,,resmlp_big_24_224_random,random,,,,,,,,,
semnasnet_100,random,,semnasnet_100,Convolutional,Semantic,SemNASNet100,"SemNASNet100 randomly initialized, with no training.",timm,https://github.com/rwightman/pytorch-image-models/,,semnasnet_100_random,random,,,,,,,,,
seresnext50_32x4d,random,,seresnext50_32x4d,Convolutional,Semantic,SEResNext50-32x4D,"SEResNext50-32x4D randomly initialized, with no training.",timm,https://github.com/rwightman/pytorch-image-models/,,seresnext50_32x4d_random,random,,,,,,,,,
skresnext50_32x4d,random,,skresnext50_32x4d,Convolutional,Semantic,SKResNext50-32x4D,"SKResNext50-32x4D randomly initialized, with no training.",timm,https://github.com/rwightman/pytorch-image-models/,,skresnext50_32x4d_random,random,,,,,,,,,
swin_base_patch4_window7_224,random,,swin_base_patch4_window7_224,Transformer,Semantic,Swin-B-P4-W7,"Swin-B-P4-W7 randomly initialized, with no training.",timm,https://github.com/rwightman/pytorch-image-models/,,swin_base_patch4_window7_224_random,random,,,,,,,,,
swin_large_patch4_window7_224,random,,swin_large_patch4_window7_224,Transformer,Semantic,Swin-L-P4-W7,"Swin-L-P4-W7 randomly initialized, with no training.",timm,https://github.com/rwightman/pytorch-image-models/,,swin_large_patch4_window7_224_random,random,,,,,,,,,
swin_tiny_patch4_window7_224,random,,swin_tiny_patch4_window7_224,Transformer,Semantic,Swin-T-P4-W7,"Swin-T-P4-W7 randomly initialized, with no training.",timm,https://github.com/rwightman/pytorch-image-models/,,swin_tiny_patch4_window7_224_random,random,,,,,,,,,
tnt_s_patch16_224,random,,tnt_s_patch16_224,Transformer,Semantic,TnT-P16-224,"TnT-P16-224 randomly initialized, with no training.",timm,https://github.com/rwightman/pytorch-image-models/,,tnt_s_patch16_224_random,random,,,,,,,,,
visformer_small,random,,visformer_small,Transformer,Semantic,Visformer,"Visformer randomly initialized, with no training.",timm,https://github.com/rwightman/pytorch-image-models/,,visformer_small_random,random,,,,,,,,,
vit_large_patch16_224,random,,vit_large_patch16_224,Transformer,Semantic,ViT-L-P16,"ViT-L-P16 randomly initialized, with no training.",timm,https://github.com/rwightman/pytorch-image-models/,,vit_large_patch16_224_random,random,,,,,,,,,
vit_small_patch16_224,random,,vit_small_patch16_224,Transformer,Semantic,ViT-S-P16,"ViT-S-P16 randomly initialized, with no training.",timm,https://github.com/rwightman/pytorch-image-models/,,vit_small_patch16_224_random,random,,,,,,,,,
vit_tiny_patch16_224,random,,vit_tiny_patch16_224,Transformer,Semantic,ViT-T-P16,"ViT-T-P16 randomly initialized, with no training.",timm,https://github.com/rwightman/pytorch-image-models/,,vit_tiny_patch16_224_random,random,,,,,,,,,
vit_base_patch16_224,random,,vit_base_patch16_224,Transformer,Semantic,ViT-B-P16,"ViT-B-P16 randomly initialized, with no training.",timm,https://github.com/rwightman/pytorch-image-models/,,vit_base_patch16_224_random,random,,,,,,,,,
vit_base_patch32_224,random,,vit_base_patch32_224,Transformer,Semantic,ViT-B-P32,"ViT-B-P32 randomly initialized, with no training.",timm,https://github.com/rwightman/pytorch-image-models/,,vit_base_patch32_224_random,random,,,,,,,,,
vit_small_patch32_224,random,,vit_small_patch32_224,Transformer,Semantic,ViT-S-P32,"ViT-S-P32 randomly initialized, with no training.",timm,https://github.com/rwightman/pytorch-image-models/,,vit_small_patch32_224_random,random,,,,,,,,,
xception,random,,xception,Convolutional,Semantic,XCeption,"XCeption randomly initialized, with no training.",timm,https://github.com/rwightman/pytorch-image-models/,,xception_random,random,,,,,,,,,
xcit_nano_12_p8_224,random,,xcit_nano_12_p8_224,Transformer,Semantic,XCIT-N-12-P8,"XCIT-N-12-P8 randomly initialized, with no training.",timm,https://github.com/rwightman/pytorch-image-models/,,xcit_nano_12_p8_224_random,random,,,,,,,,,
xcit_nano_12_p16_224,random,,xcit_nano_12_p16_224,Transformer,Semantic,XCIT-N-12-P16,"XCIT-N-12-P16 randomly initialized, with no training.",timm,https://github.com/rwightman/pytorch-image-models/,,xcit_nano_12_p16_224_random,random,,,,,,,,,
RN50,clip,openai400M,ResNet50,Convolutional,Vision-Language,CLiP-ResNet50,"CLiP-ResNet50, a hybrid vision-language model.",clip,https://github.com/openai/CLIP,,RN50_clip,,,,,,,,,,
RN101,clip,openai400M,ResNet101,Convolutional,Vision-Language,CLiP-ResNet101,"CLiP-ResNet101, a hybrid vision-language model.",clip,https://github.com/openai/CLIP,,RN101_clip,,,,,,,,,,
ViT-B/32,clip,openai400M,ViT-B/32,Transformer,Vision-Language,CLiP-ViT-B/32,"CLiP-ViT-B/32, a hybrid vision-language model.",clip,https://github.com/openai/CLIP,,ViT-B/32_clip,,,,,,,,,,
ViT-B/16,clip,openai400M,ViT-B/16,Transformer,Vision-Language,CLiP-ViT-B/32,"CLiP-ViT-B/32, a hybrid vision-language model.",clip,https://github.com/openai/CLIP,,ViT-B/16_clip,,,,,,,,,,
ViT-L/14,clip,openai400M,ViT-L/14,Transformer,Vision-Language,CLiP-ViT-L/14,"CLiP-ViT-L/14, a hybrid vision-language model.",clip,https://github.com/openai/CLIP,,ViT-L/14_clip,,,,,,,,,,
ResNet50-JigSaw-P100,selfsupervised,imagenet,ResNet50,Convolutional,Self-Supervised,ResNet50-JigSaw-P100,"ResNet50-JigSaw-P100, a self-supervised representation learner trained on ImageNet.",vissl,https://github.com/facebookresearch/vissl/,https://dl.fbaipublicfiles.com/vissl/model_zoo/jigsaw_rn50_in1k_ep105_perm2k_jigsaw_8gpu_resnet_17_07_20.db174a43/model_final_checkpoint_phase104.torch,ResNet50-JigSaw-P100_selfsupervised,,,,,,,,JigSaw-P100,Non-Contrastive,
ResNet50-JigSaw-Goyal19,selfsupervised,imagenet,ResNet50,Convolutional,Self-Supervised,ResNet50-JigSaw-Goyal19,"ResNet50-JigSaw-Goyal19, a self-supervised representation learner trained on ImageNet.",vissl,https://github.com/facebookresearch/vissl/,https://dl.fbaipublicfiles.com/vissl/model_zoo/converted_vissl_rn50_jigsaw_in1k_goyal19.torch,ResNet50-JigSaw-Goyal19_selfsupervised,,,,,,,,JigSaw-Goyal19,Non-Contrastive,
ResNet50-RotNet,selfsupervised,imagenet,ResNet50,Convolutional,Self-Supervised,ResNet50-RotNet,"ResNet50-RotNet, a self-supervised representation learner trained on ImageNet.",vissl,https://github.com/facebookresearch/vissl/,https://dl.fbaipublicfiles.com/vissl/model_zoo/rotnet_rn50_in1k_ep105_rotnet_8gpu_resnet_17_07_20.46bada9f/model_final_checkpoint_phase125.torch,ResNet50-RotNet_selfsupervised,,,,,,,,RotNet,Non-Contrastive,
ResNet50-ClusterFit-16K-RotNet,selfsupervised,imagenet,ResNet50,Convolutional,Self-Supervised,ResNet50-ClusterFit-16K-RotNet,"ResNet50-ClusterFit-16K-RotNet, a self-supervised representation learner trained on ImageNet.",vissl,https://github.com/facebookresearch/vissl/,https://dl.fbaipublicfiles.com/vissl/model_zoo/converted_vissl_rn50_rotnet_16kclusters_in1k_ep105.torch,ResNet50-ClusterFit-16K-RotNet_selfsupervised,,,,,,,,ClusterFit-16K-RotNet,Non-Contrastive,
ResNet50-PIRL,selfsupervised,imagenet,ResNet50,Convolutional,Self-Supervised,ResNet50-PIRL,"ResNet50-PIRL, a self-supervised representation learner trained on ImageNet.",vissl,https://github.com/facebookresearch/vissl/,https://dl.fbaipublicfiles.com/vissl/model_zoo/pirl_jigsaw_4node_pirl_jigsaw_4node_resnet_22_07_20.34377f59/model_final_checkpoint_phase799.torch,ResNet50-PIRL_selfsupervised,,,,,,,,PIRL,Contrastive,
ResNet50-SimCLR,selfsupervised,imagenet,ResNet50,Convolutional,Self-Supervised,ResNet50-SimCLR,"ResNet50-SimCLR, a self-supervised representation learner trained on ImageNet.",vissl,https://github.com/facebookresearch/vissl/,https://dl.fbaipublicfiles.com/vissl/model_zoo/simclr_rn50_1000ep_simclr_8node_resnet_16_07_20.afe428c7/model_final_checkpoint_phase999.torch,ResNet50-SimCLR_selfsupervised,,,,,,,,SimCLR,Contrastive,
ResNet50-DeepClusterV2-2x224+6x96,selfsupervised,imagenet,ResNet50,Convolutional,Self-Supervised,ResNet50-DeepClusterV2,"ResNet50-DeepClusterV2-2x224+6x96, a self-supervised representation learner trained on ImageNet.",vissl,https://github.com/facebookresearch/vissl/,https://dl.fbaipublicfiles.com/vissl/model_zoo/deepclusterv2_800ep_pretrain.pth.tar,ResNet50-DeepClusterV2-2x224+6x96_selfsupervised,,,,,,,,DeepClusterV2-2x224+6x96,Contrastive,
ResNet50-SwAV-BS4096-2x224+6x96,selfsupervised,imagenet,ResNet50,Convolutional,Self-Supervised,ResNet50-SwAV-BS4096,"ResNet50-SwAV-BS4096-2x224+6x96, a self-supervised representation learner trained on ImageNet.",vissl,https://github.com/facebookresearch/vissl/,https://dl.fbaipublicfiles.com/vissl/model_zoo/swav_8node_2x224_rn50_in1k_swav_8node_resnet_30_07_20.c8fd7169/model_final_checkpoint_phase399.torch,ResNet50-SwAV-BS4096-2x224+6x96_selfsupervised,,,,,,,,SwAV-BS4096-2x224+6x96,Contrastive,
ResNet50-MoCoV2-BS256,selfsupervised,imagenet,ResNet50,Convolutional,Self-Supervised,ResNet50-MoCoV2-BS256,"ResNet50-MoCoV2-BS256, a self-supervised representation learner trained on ImageNet.",vissl,https://github.com/facebookresearch/vissl/,https://dl.fbaipublicfiles.com/vissl/model_zoo/moco_v2_1node_lr.03_step_b32_zero_init/model_final_checkpoint_phase199.torch,ResNet50-MoCoV2-BS256_selfsupervised,,,,,,,,MoCoV2-BS256,Contrastive,
ResNet50-BarlowTwins-BS2048,selfsupervised,imagenet,ResNet50,Convolutional,Self-Supervised,ResNet50-BarlowTwins-BS2048,"ResNet50-BarlowTwins-BS2048, a self-supervised representation learner trained on ImageNet.",vissl,https://github.com/facebookresearch/vissl/, https://dl.fbaipublicfiles.com/vissl/model_zoo/barlow_twins/barlow_twins_32gpus_4node_imagenet1k_1000ep_resnet50.torch,ResNet50-BarlowTwins-BS2048_selfsupervised,,,,,,,,BarlowTwins-BS2048,Contrastive,
dino_vitb16,selfsupervised,imagenet,vitb16,Transformer,SelfSupervised,Dino-VIT-B16,Dino-VIT-B16 trained via self supervision with the ImageNet dataset.,dino,https://github.com/facebookresearch/dino,,dino_vitb16_selfsupervised,,,,,,,,,,
dino_resnet50,selfsupervised,imagenet,resnet50,Convolutional,SelfSupervised,Dino-ResNet50,Dino-ResNet50 trained via self supervision with the ImageNet dataset.,dino,https://github.com/facebookresearch/dino,,dino_resnet50_selfsupervised,,,,,,,,,,
DPT_Hybrid,monoculardepth,"ReDWeb,DIML,Movies,MegaDepth,WSVD,TartanAir,HRWSI,ApolloScape,BlendedMVS,IRS",DPT_Hybrid,Transformer,MonocularDepth,DPT-Hybrid,"DPT-Hybrid, a monocular depth estimation model.",midas,https://github.com/isl-org/MiDaS,,DPT_Hybrid_monoculardepth,,,,,,,,,,
MiDaS,monoculardepth,"ReDWeb,DIML,Movies,MegaDepth,WSVD,TartanAir,HRWSI,ApolloScape,BlendedMVS,IRS",MiDaS,,MonocularDepth,MiDaS,"MiDaS, a monocular depth estimation model.",midas,https://github.com/isl-org/MiDaS,,MiDaS_monoculardepth,,,,,,,,,,
yolov5l,yolo,"coco,voc",yolov5l,Convolutional,Detection,YOLO-V5-L,"YOLO-V5-L, an object detection model.",yolo,https://github.com/ultralytics/yolov5,,yolov5l_yolo,,,,,,,,,,
yolov5m,yolo,"coco,voc",yolov5m,Convolutional,Detection,YOLO-V5-M,"YOLO-V5-M, an object detection model.",yolo,https://github.com/ultralytics/yolov5,,yolov5m_yolo,,,,,,,,,,
yolov5s,yolo,"coco,voc",yolov5s,Convolutional,Detection,YOLO-V5-S,"YOLO-V5-S, an object detection model.",yolo,https://github.com/ultralytics/yolov5,,yolov5s_yolo,,,,,,,,,,
faster_rcnn_R_50_FPN_3x,detection,coco2017,faster_rcnn_R_50_FPN_3x,Convolutional,Detection|Segmentation,Faster-RCNN-ResNet50-FPN,"Faster-RCNN-ResNet50-FPN, trained on detection with the CoCo2017 dataset.",detectron,https://github.com/facebookresearch/detectron2/,COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml,faster_rcnn_R_50_FPN_3x_detection,,,,,,,,,,
retinanet_R_50_FPN_3x,detection,coco2017,retinanet_R_50_FPN_3x,Convolutional,Detection|Segmentation,RetinaNet-ResNet50-FPN,"RetinaNet-ResNet50-FPN, trained on detection with the CoCo2017 dataset.",detectron,https://github.com/facebookresearch/detectron2/,COCO-Detection/retinanet_R_50_FPN_3x.yaml,retinanet_R_50_FPN_3x_detection,,,,,,,,,,
mask_rcnn_R_50_FPN_3x,segmentation,coco2017,mask_rcnn_R_50_FPN_3x,Convolutional,Detection|Segmentation,Mask-RCNN-ResNet50-FPN,"Mask-RCNN-ResNet50-FPN, trained on segmentation with the CoCo2017 dataset.",detectron,https://github.com/facebookresearch/detectron2/,COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml,mask_rcnn_R_50_FPN_3x_segmentation,,,,,,,,,,
keypoint_rcnn_R_50_FPN_3x,segmentation,coco2017,keypoint_rcnn_R_50_FPN_3x,Convolutional,Detection|Segmentation,Keypoint-RCNN-ResNet50-FPN,"Keypoint-RCNN-ResNet50-FPN, trained on segmentation with the CoCo2017 dataset.",detectron,https://github.com/facebookresearch/detectron2/,COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml,keypoint_rcnn_R_50_FPN_3x_segmentation,,,,,,,,,,
convnext_base_in22k,classification,imagenet21k,convnext_base,Convolutional,Semantic,ConvNext-Base-IN21K,ConvNext-Base-IN21K trained on image classification with the ImageNet21K dataset.,timm,https://github.com/rwightman/pytorch-image-models/,,convnext_base_in22k_classification,,,,,,,,,,imagenet21k
convnext_large_in22k,classification,imagenet21k,convnext_large,Convolutional,Semantic,ConvNext-Large-IN21K,ConvNext-Large-IN21K trained on image classification with the ImageNet21K dataset.,timm,https://github.com/rwightman/pytorch-image-models/,,convnext_large_in22k_classification,,,,,,,,,,imagenet21k
mixer_b16_224_in21k,classification,imagenet21k,mixer_b16_224,MLP-Mixer,Semantic,Mixer-B16-IN22K,Mixer-B16-IN22K trained on image classification with the ImageNet21K dataset.,timm,https://github.com/rwightman/pytorch-image-models/,,mixer_b16_224_in21k_classification,,,,,,,,,,imagenet21k
mixer_l16_224_in21k,classification,imagenet21k,mixer_l16_224,MLP-Mixer,Semantic,Mixer-L16-IN22K,Mixer-L16-IN22K trained on image classification with the ImageNet21K dataset.,timm,https://github.com/rwightman/pytorch-image-models/,,mixer_l16_224_in21k_classification,,,,,,,,,,imagenet21k
swin_base_patch4_window7_224_in22k,classification,imagenet21k,swin_base_patch4_window7_224,Transformer,Semantic,Swin-B-P4-W7-IN21K,Swin-B-P4-W7-IN21K trained on image classification with the ImageNet21K dataset.,timm,https://github.com/rwightman/pytorch-image-models/,,swin_base_patch4_window7_224_in22k_classification,,,,,,,,,,imagenet21k
swin_large_patch4_window7_224_in22k,classification,imagenet21k,swin_large_patch4_window7_224,Transformer,Semantic,Swin-L-P4-W7-IN21K,Swin-L-P4-W7-IN21K trained on image classification with the ImageNet21K dataset.,timm,https://github.com/rwightman/pytorch-image-models/,,swin_large_patch4_window7_224_in22k_classification,,,,,,,,,,imagenet21k
resmlp_big_24_224_in22ft1k,classification,imagenet21k,resmlp_big_24_224,MLP-Mixer,Semantic,ResMLP-Big-24-IN21K,ResMLP-Big-24-IN21K trained on image classification with the ImageNet21K dataset.,timm,https://github.com/rwightman/pytorch-image-models/,,resmlp_big_24_224_in22ft1k_classification,,,,,,,,,,imagenet21k
vit_base_r50_s16_224_in21k,classification,imagenet21k,vit_base_r50_s16_224,Transformer,Semantic,ViT-B-R50-S16-IN21K,ViT-B-R50-S16-IN21K trained on image classification with the ImageNet21K dataset.,timm,https://github.com/rwightman/pytorch-image-models/,,vit_base_r50_s16_224_in21k_classification,,,,,,,,,,
vit_base_patch16_224_in21k,classification,imagenet21k,vit_base_patch16_224,Transformer,Semantic,ViT-B-P16-IN21K,ViT-B-P16-IN21K trained on image classification with the ImageNet21K dataset.,timm,https://github.com/rwightman/pytorch-image-models/,,vit_base_patch16_224_in21k_classification,,,,,,,,,,imagenet21k
vit_small_patch16_224_in21k,classification,imagenet21k,vit_small_patch16_224,Transformer,Semantic,ViT-S-P16-IN21K,ViT-S-P16-IN21K trained on image classification with the ImageNet21K dataset.,timm,https://github.com/rwightman/pytorch-image-models/,,vit_small_patch16_224_in21k_classification,,,,,,,,,,imagenet21k
vit_large_patch16_224_in21k,classification,imagenet21k,vit_large_patch16_224,Transformer,Semantic,ViT-L-P16-IN21K,ViT-L-P16-IN21K trained on image classification with the ImageNet21K dataset.,timm,https://github.com/rwightman/pytorch-image-models/,,vit_large_patch16_224_in21k_classification,,,,,,,,,,imagenet21k
vit_base_patch32_224_in21k,classification,imagenet21k,vit_base_patch32_224,Transformer,Semantic,ViT-B-P32-IN21K,ViT-B-P32-IN21K trained on image classification with the ImageNet21K dataset.,timm,https://github.com/rwightman/pytorch-image-models/,,vit_base_patch32_224_in21k_classification,,,,,,,,,,imagenet21k
vit_small_patch32_224_in21k,classification,imagenet21k,vit_small_patch32_224,Transformer,Semantic,ViT-S-P32-IN21K,ViT-S-P32-IN21K trained on image classification with the ImageNet21K dataset.,timm,https://github.com/rwightman/pytorch-image-models/,,vit_small_patch32_224_in21k_classification,,,,,,,,,,imagenet21k
ViT-S-SimCLR,slip,YFCC15M,ViT-S,Transformer,SelfSupervised,ViT-S-SimCLR,ViT-S-SimCLR trained via pure self-supervision with the YFCC15M dataset.,slip,https://github.com/facebookresearch/slip,,ViT-S-SimCLR_slip,,,SimCLR,,,,,,,
ViT-S-CLIP,slip,YFCC15M,ViT-S,Transformer,Vision-Language,ViT-S-CLIP,ViT-S-CLIP trained via pure language supervision with the YFCC15M dataset.,slip,https://github.com/facebookresearch/slip,,ViT-S-CLIP_slip,,,CLIP,,,,,,,
ViT-S-SLIP,slip,YFCC15M,ViT-S,Transformer,Vision-Language,ViT-S-SLIP,ViT-S-SLIP trained via combined self- and language supervision with the YFCC15M dataset.,slip,https://github.com/facebookresearch/slip,,ViT-S-SLIP_slip,,,SLIP,,,,,,,
ViT-B-SimCLR,slip,YFCC15M,ViT-B,Transformer,SelfSupervised,ViT-B-SimCLR,ViT-B-SimCLR trained via pure self-supervision with the YFCC15M dataset.,slip,https://github.com/facebookresearch/slip,,ViT-B-SimCLR_slip,,,SimCLR,,,,,,,
ViT-B-CLIP,slip,YFCC15M,ViT-B,Transformer,Vision-Language,ViT-B-CLIP,ViT-B-CLIP trained via pure language supervision with the YFCC15M dataset.,slip,https://github.com/facebookresearch/slip,,ViT-B-CLIP_slip,,,CLIP,,,,,,,
ViT-B-SLIP,slip,YFCC15M,ViT-B,Transformer,Vision-Language,ViT-B-SLIP,ViT-B-SLIP trained via combined self- and language supervision with the YFCC15M dataset.,slip,https://github.com/facebookresearch/slip,,ViT-B-SLIP_slip,,,SLIP,,,,,,,
ViT-L-SimCLR,slip,YFCC15M,ViT-L,Transformer,SelfSupervised,ViT-L-SimCLR,ViT-L-SimCLR trained via pure self-supervision with the YFCC15M dataset.,slip,https://github.com/facebookresearch/slip,,ViT-L-SimCLR_slip,,,SimCLR,,,,,,,
ViT-L-CLIP,slip,YFCC15M,ViT-L,Transformer,Vision-Language,ViT-L-CLIP,ViT-L-CLIP trained via pure language supervision with the YFCC15M dataset.,slip,https://github.com/facebookresearch/slip,,ViT-L-CLIP_slip,,,CLIP,,,,,,,
ViT-L-SLIP,slip,YFCC15M,ViT-L,Transformer,Vision-Language,ViT-L-SLIP,ViT-L-SLIP trained via combined self- and language supervision with the YFCC15M dataset.,slip,https://github.com/facebookresearch/slip,,ViT-L-SLIP_slip,,,SLIP,,,,,,,
ViT-L-CLIP-CC12M,slip,YFCC15M,ViT-L,Transformer,Vision-Language,ViT-L-CLIP-CC12M,ViT-L-CLIP-CC12M trained via pure language supervision with the YFCC15M dataset.,slip,https://github.com/facebookresearch/slip,,ViT-L-CLIP-CC12M_slip,,,,,,,,,,
ViT-L-SLIP-CC12M,slip,YFCC15M,ViT-L,Transformer,Vision-Language,ViT-L-SLIP-CC12M,ViT-L-SLIP-CC12M trained via combined self- and language supervision with the YFCC15M dataset.,slip,https://github.com/facebookresearch/slip,,ViT-L-SLIP-CC12M_slip,,,,,,,,,,
RegNet-32Gf-SEER,seer,random1B,RegNet-32Gf,Convolutional,SelfSupervised,RegNet-32Gf-SEER,RegNet-32Gf trained via large-scale self-supervision on 1 billion images.,seer,https://github.com/facebookresearch/vissl/blob/main/MODEL_ZOO.md#seer,,RegNet-32Gf-SEER_seer,,,,,,,,,,
RegNet-32Gf-SEER-INFT,seer,random1B,RegNet-32Gf,Convolutional,SelfSupervised,RegNet-32Gf-SEER-INFT,RegNet-32Gf trained via large-scale self-supervision on 1 billion images.,seer,https://github.com/facebookresearch/vissl/blob/main/MODEL_ZOO.md#seer,,RegNet-32Gf-SEER-INFT_seer,,,,,,,,,,
RegNet-64Gf-SEER,seer,random1B,RegNet-64Gf,Convolutional,SelfSupervised,RegNet-64Gf-SEER,RegNet-64Gf trained via large-scale self-supervision on 1 billion images.,seer,https://github.com/facebookresearch/vissl/blob/main/MODEL_ZOO.md#seer,,RegNet-64Gf-SEER_seer,,,,,,,,,,
RegNet-64Gf-SEER-INFT,seer,random1B,RegNet-64Gf,Convolutional,SelfSupervised,RegNet-64Gf-SEER-INFT,RegNet-64Gf trained via large-scale self-supervision on 1 billion images.,seer,https://github.com/facebookresearch/vissl/blob/main/MODEL_ZOO.md#seer,,RegNet-64Gf-SEER-INFT_seer,,,,,,,,,,
RegNet-128Gf-SEER,seer,random1B,RegNet-128Gf,Convolutional,SelfSupervised,RegNet-128Gf-SEER,RegNet-128Gf trained via large-scale self-supervision on 1 billion images.,seer,https://github.com/facebookresearch/vissl/blob/main/MODEL_ZOO.md#seer,,RegNet-128Gf-SEER_seer,,,,,,,,,,
RegNet-128Gf-SEER-INFT,seer,random1B,RegNet-128Gf,Convolutional,SelfSupervised,RegNet-128Gf-SEER-INFT,RegNet-128Gf trained via large-scale self-supervision on 1 billion images.,seer,https://github.com/facebookresearch/vissl/blob/main/MODEL_ZOO.md#seer,,RegNet-128Gf-SEER-INFT_seer,,,,,,,,,,
alexnet_gn_ipcl_imagenet,ipcl,imagenet,alexnet_gn,Convolutional,,AlexNet-GN-IPCLImageNet,AlexNet (modified with GroupNorm) trained via IPCL-Style-Self-Supervision on ImageNet.,open_ipcl,https://github.com/harvard-visionlab/open_ipcl,,alexnet_gn_ipcl_imagenet_ipcl,,,,,,,imagenet,,,
alexnet_gn_ipcl_openimages,ipcl,openimages,alexnet_gn,Convolutional,,AlexNet-GN-IPCLOpenImages,AlexNet (modified with GroupNorm) trained via IPCL-Style-Self-Supervision on OpenImages.,open_ipcl,https://github.com/harvard-visionlab/open_ipcl,,alexnet_gn_ipcl_openimages_ipcl,,,,,,,openimages,,,
alexnet_gn_ipcl_places256,ipcl,places256,alexnet_gn,Convolutional,,AlexNet-GN-IPCLPlaces256,AlexNet (modified with GroupNorm) trained via IPCL-Style-Self-Supervision on Places256.,open_ipcl,https://github.com/harvard-visionlab/open_ipcl,,alexnet_gn_ipcl_places256_ipcl,,,,,,,places256,,,
alexnet_gn_ipcl_vggface2,ipcl,vggface2,alexnet_gn,Convolutional,,AlexNet-GN-IPCLVGGFace2,AlexNet (modified with GroupNorm) trained via IPCL-Style-Self-Supervision on VGGFace2.,open_ipcl,https://github.com/harvard-visionlab/open_ipcl,,alexnet_gn_ipcl_vggface2_ipcl,,,,,,,vggface2,,,
BiT-Expert-ResNet-V2-Food,bit_expert,big_transfer,ResNet50-V2,Convolutional,Expertise,BiT-Expert-ResNet-V2-Food,"ResNet50-V2 trained first via ImageNet21K dataset, then as an expert on the Food subset.",bit_expert,https://tfhub.dev/google/collections/experts/bit/1,,BiT-Expert-ResNet-V2-Food_bit_expert,,,,,,Food,,,,
BiT-Expert-ResNet-V2-Vehicle,bit_expert,big_transfer,ResNet50-V2,Convolutional,Expertise,BiT-Expert-ResNet-V2-Vehicle,"ResNet50-V2 trained first via ImageNet21K dataset, then as an expert on the Vehicle subset.",bit_expert,https://tfhub.dev/google/collections/experts/bit/1,,BiT-Expert-ResNet-V2-Vehicle_bit_expert,,,,,,Vehicle,,,,
BiT-Expert-ResNet-V2-Instrument,bit_expert,big_transfer,ResNet50-V2,Convolutional,Expertise,BiT-Expert-ResNet-V2-Instrument,"ResNet50-V2 trained first via ImageNet21K dataset, then as an expert on the Instrument subset.",bit_expert,https://tfhub.dev/google/collections/experts/bit/1,,BiT-Expert-ResNet-V2-Instrument_bit_expert,,,,,,Instrument,,,,
BiT-Expert-ResNet-V2-Flower,bit_expert,big_transfer,ResNet50-V2,Convolutional,Expertise,BiT-Expert-ResNet-V2-Flower,"ResNet50-V2 trained first via ImageNet21K dataset, then as an expert on the Flower subset.",bit_expert,https://tfhub.dev/google/collections/experts/bit/1,,BiT-Expert-ResNet-V2-Flower_bit_expert,,,,,,Flower,,,,
BiT-Expert-ResNet-V2-Animal,bit_expert,big_transfer,ResNet50-V2,Convolutional,Expertise,BiT-Expert-ResNet-V2-Animal,"ResNet50-V2 trained first via ImageNet21K dataset, then as an expert on the Animal subset.",bit_expert,https://tfhub.dev/google/collections/experts/bit/1,,BiT-Expert-ResNet-V2-Animal_bit_expert,,,,,,Animal,,,,
BiT-Expert-ResNet-V2-Object,bit_expert,big_transfer,ResNet50-V2,Convolutional,Expertise,BiT-Expert-ResNet-V2-Object,"ResNet50-V2 trained first via ImageNet21K dataset, then as an expert on the Object subset.",bit_expert,https://tfhub.dev/google/collections/experts/bit/1,,BiT-Expert-ResNet-V2-Object_bit_expert,,,,,,Object,,,,
BiT-Expert-ResNet-V2-Bird,bit_expert,big_transfer,ResNet50-V2,Convolutional,Expertise,BiT-Expert-ResNet-V2-Bird,"ResNet50-V2 trained first via ImageNet21K dataset, then as an expert on the Bird subset.",bit_expert,https://tfhub.dev/google/collections/experts/bit/1,,BiT-Expert-ResNet-V2-Bird_bit_expert,,,,,,Bird,,,,
BiT-Expert-ResNet-V2-Mammal,bit_expert,big_transfer,ResNet50-V2,Convolutional,Expertise,BiT-Expert-ResNet-V2-Mammal,"ResNet50-V2 trained first via ImageNet21K dataset, then as an expert on the Mammal subset.",bit_expert,https://tfhub.dev/google/collections/experts/bit/1,,BiT-Expert-ResNet-V2-Mammal_bit_expert,,,,,,Mammal,,,,
BiT-Expert-ResNet-V2-Arthropod,bit_expert,big_transfer,ResNet50-V2,Convolutional,Expertise,BiT-Expert-ResNet-V2-Arthropod,"ResNet50-V2 trained first via ImageNet21K dataset, then as an expert on the Arthropod subset.",bit_expert,https://tfhub.dev/google/collections/experts/bit/1,,BiT-Expert-ResNet-V2-Arthropod_bit_expert,,,,,,Arthropod,,,,
BiT-Expert-ResNet-V2-Relation,bit_expert,big_transfer,ResNet50-V2,Convolutional,Expertise,BiT-Expert-ResNet-V2-Relation,"ResNet50-V2 trained first via ImageNet21K dataset, then as an expert on the Relation subset.",bit_expert,https://tfhub.dev/google/collections/experts/bit/1,,BiT-Expert-ResNet-V2-Relation_bit_expert,,,,,,Relation,,,,
BiT-Expert-ResNet-V2-Abstraction,bit_expert,big_transfer,ResNet50-V2,Convolutional,Expertise,BiT-Expert-ResNet-V2-Abstraction,"ResNet50-V2 trained first via ImageNet21K dataset, then as an expert on the Abstraction subset.",bit_expert,https://tfhub.dev/google/collections/experts/bit/1,,BiT-Expert-ResNet-V2-Abstraction_bit_expert,,,,,,Abstraction,,,,
